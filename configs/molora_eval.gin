# ginlint: disable=bad-import-order
from __gin__ import dynamic_registration

import seqio
from t5x import models
from t5x import utils
from t5x import adafactor
from t5x import partitioning
from t5x import optimizers as optim
from flax import linen
from flax import traverse_util
from flaxformer.components import dense
from flaxformer.components.attention import dense_attention

from src import adafactor_custom as c_optim
from src import partitioning_custom as c_partitioning

from src import utils as peft_utils
from src import routing
from src import molora

include 't5x/configs/runs/eval.gin'

# ========== Data Mixture ==========
# SeqIO tasks for p3 (from original repo)
import t0_data

# ========== These are IA3 HPs you might want to override ==========
# If you want to change the actual optimizer itself (to optim.Adam, etc), make
# sure to update the optimizer that is passed to the MultiOptimizer.
adafactor.Adafactor:
  decay_rate = 0.8
  step_offset = 0
  logical_factor_rules = @c_optim.standard_logical_factor_rules()

# ========== These are IA3 HPs you might want to override ==========
partitioning.PjitPartitioner:
  logical_axis_rules = @partitioning.standard_logical_axis_rules()

partitioning.standard_logical_axis_rules:
  additional_rules = @c_partitioning.standard_logical_axis_rules()

# ========== ARCHITECTURE ==========
dense_attention.MultiHeadDotProductAttention:
  lora_output_conv = @attn/molora.MoLoRa()
  lora_q_conv = @attn/molora.MoLoRaAttention()
  lora_k_conv = @attn/molora.MoLoRaAttention()
  lora_v_conv = @attn/molora.MoLoRaAttention()

attn/molora.MoLoRaAttention:
  rank = 4
  num_experts = 10
  num_heads = 16
  router = @attn/routing.Router()
  dtype = 'bfloat16'
  lora_axis_names_A = ('expert', 'embed', 'rank')
  lora_axis_names_B = ('expert', 'rank', 'embed')

attn/molora.MoLoRa:
  rank = 4
  num_experts = 10
  router = @attn_out/routing.Router()
  dtype = 'bfloat16'
  lora_axis_names_A = ('expert', 'embed', 'rank')
  lora_axis_names_B = ('expert', 'rank', 'embed')

dense.MlpBlock:
  lora_intermediate_conv = @mlp1/molora.MoLoRa()
  lora_output_conv = @mlp2/molora.MoLoRa()

mlp1/molora.MoLoRa:
  rank = 4
  num_experts = 10
  output_dim = 2816
  router = @mlp1/routing.Router()
  dtype = 'bfloat16'
  lora_axis_names_A = ('expert', 'embed', 'rank')
  lora_axis_names_B = ('expert', 'rank', 'mlp')

mlp2/molora.MoLoRa:
  rank = 4
  num_experts = 10
  output_dim = 1024
  router = @mlp2/routing.Router()
  dtype = 'bfloat16'
  lora_axis_names_A = ('expert', 'mlp', 'rank')
  lora_axis_names_B = ('expert', 'rank', 'embed')

attn/routing.Router:
  router_weights = @attn/routing.RouterWeights()
  input_axis_names = ('batch', 'length', 'embed')
  jitter_noise = 0.0
  dtype = 'float32'
  ignore_padding_tokens = False

attn/routing.RouterWeights:
  use_bias = False
  dtype = 'float32'
  kernel_axis_names = ('embed', 'expert')
  kernel_init = @router_init/linen.initializers.normal()
  bias_init = %BIAS_INIT
# We obtain slightly better results adopting typical normally-distributed
# scaling for the router, rather than the 0.1-scaled variance_scaling. May be
# worth revisiting if stability becomes an issue during training.
router_init/linen.initializers.normal:
  stddev = 2e-2

attn_out/routing.Router:
  router_weights = @attn_out/routing.RouterWeights()
  input_axis_names = ('batch', 'length', 'embed')
  jitter_noise = 0.0
  dtype = 'float32'
  ignore_padding_tokens = False

attn_out/routing.RouterWeights:
  use_bias = False
  dtype = 'float32'
  kernel_axis_names = ('embed', 'expert')
  kernel_init = @router_init/linen.initializers.normal()
  bias_init = %BIAS_INIT
# We obtain slightly better results adopting typical normally-distributed
# scaling for the router, rather than the 0.1-scaled variance_scaling. May be
# worth revisiting if stability becomes an issue during training.
router_init/linen.initializers.normal:
  stddev = 2e-2

mlp1/routing.Router:
  router_weights = @mlp1/routing.RouterWeights()
  input_axis_names = ('batch', 'length', 'embed')
  jitter_noise = 0.0
  dtype = 'float32'
  ignore_padding_tokens = False

mlp1/routing.RouterWeights:
  use_bias = False
  dtype = 'float32'
  kernel_axis_names = ('embed', 'expert')
  kernel_init = @router_init/linen.initializers.normal()
  bias_init = %BIAS_INIT
# We obtain slightly better results adopting typical normally-distributed
# scaling for the router, rather than the 0.1-scaled variance_scaling. May be
# worth revisiting if stability becomes an issue during training.
router_init/linen.initializers.normal:
  stddev = 2e-2

mlp2/routing.Router:
  router_weights = @mlp2/routing.RouterWeights()
  input_axis_names = ('batch', 'length', 'mlp')
  jitter_noise = 0.0
  dtype = 'float32'
  ignore_padding_tokens = False

mlp2/routing.RouterWeights:
  use_bias = False
  dtype = 'float32'
  kernel_axis_names = ('mlp', 'expert')
  kernel_init = @router_init/linen.initializers.normal()
  bias_init = %BIAS_INIT
# We obtain slightly better results adopting typical normally-distributed
# scaling for the router, rather than the 0.1-scaled variance_scaling. May be
# worth revisiting if stability becomes an issue during training.
router_init/linen.initializers.normal:
  stddev = 2e-2