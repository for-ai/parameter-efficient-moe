# ginlint: disable=bad-import-order
from __gin__ import dynamic_registration

import seqio
from t5x import models
from t5x import utils
from t5x import adafactor
from t5x import partitioning
from t5x import optimizers as optim
from flax import linen
from flax import traverse_util
from flaxformer.components import dense
from flaxformer.components.attention import dense_attention

from src import adafactor_custom as c_optim
from src import partitioning_custom as c_partitioning

from src import utils as peft_utils
from src import routing
from src import molora

include 't5x/configs/runs/finetune_no_eval.gin'

# ========== Data Mixture ==========
# SeqIO tasks for p3 (from original repo)
import t0_data

# ========== These are IA3 HPs you might want to override ==========
# If you want to change the actual optimizer itself (to optim.Adam, etc), make
# sure to update the optimizer that is passed to the MultiOptimizer.
adafactor.Adafactor:
  decay_rate = 0.8
  step_offset = 0
  logical_factor_rules = @c_optim.standard_logical_factor_rules()

# ========== These are IA3 HPs you might want to override ==========
partitioning.PjitPartitioner:
  logical_axis_rules = @partitioning.standard_logical_axis_rules()

partitioning.standard_logical_axis_rules:
  additional_rules = @c_partitioning.standard_logical_axis_rules()

# ========== Partial Loading ==========
# The following is the configuration the allows to partially load a model (using
# the values in a checkpoint) without it complaining that the shapes don't match
# (because we have extra parameters, the ia3 scaling values) in our model.
# You shouldn't need to update these outside of if you want to change the
# optimizer itself.
#
# Optimizer
# LR is set by `Trainer.learning_rate_fn`.
# Use our MultiOptimizer wrapper to bind to the variadic
# `*traversals_and_optimizers`
OPTIMIZER = @optim.MultiOptimizer()
optim.MultiOptimizer:
  traversals_and_optimizers = ((@traverse_util.ModelParamTraversal(),
                                @adafactor.Adafactor()),)
traverse_util.ModelParamTraversal:
  filter_fn = @peft_utils.match_any()
# Our MultiOptimzier will match any parameter with a flattened name that
# matches any of these regular expressions.
TRAINABLE_REGEX = [".*/lora_A.*", ".*/lora_B.*", ".*/router.*"]
peft_utils.match_any.regexes = %TRAINABLE_REGEX

# These settings allow us to partially reload a checkpoint, that is, we can load
# most of the model weights from the checkpoint, without it complaining that we
# don't have a weight for our ia3 scaling values in the checkpoint.
utils.RestoreCheckpointConfig:
  # Activate the codepath that allows the merging of the optimizer state as
  # specified in the config (with our new parameter) and the optimizer state as
  # defined in the checkpoint.
  fallback_to_scratch = True
  # Use the T5X assignment map to grab values from the checkpoint. Each entry in
  # the map is a regular expression that matches some flattened variable name in
  # the optimizer state as defined in the model created by the config. The
  # second value is the corresponding name in optimizer state as defined by the
  # checkpoint. It supports interpolating capture groups from the initial regex.
  # If the second pattern is `None` we skip trying to load this variable from
  # the checkpoint.

  # Skip trying to load all keys that have the word ia3_scaling in them, these
  # will be initialized from scratch.
  assignment_map = ((r"^.*lora_A.*$", None),
                    (r"^.*lora_B.*$", None),
                    (r"^.*router.*$", None),)

utils.create_learning_rate_scheduler:
  factors = "constant"
  # Learning rate from the paper.
  base_learning_rate = 3e-4

utils.SaveCheckpointConfig:
  period = 10000
  keep = 60

# ========== ARCHITECTURE ==========
dense_attention.MultiHeadDotProductAttention:
  lora_output_conv = @attn/molora.MoLoRa()
  lora_q_conv = @attn/molora.MoLoRaAttention()
  lora_k_conv = @attn/molora.MoLoRaAttention()
  lora_v_conv = @attn/molora.MoLoRaAttention()

attn/molora.MoLoRaAttention:
  rank = 4
  num_experts = 10
  num_heads = 16
  router = @attn/routing.Router()
  dtype = 'bfloat16'
  lora_axis_names_A = ('expert', 'embed', 'rank')
  lora_axis_names_B = ('expert', 'rank', 'embed')

attn/molora.MoLoRa:
  rank = 4
  num_experts = 10
  router = @attn_out/routing.Router()
  dtype = 'bfloat16'
  lora_axis_names_A = ('expert', 'embed', 'rank')
  lora_axis_names_B = ('expert', 'rank', 'embed')

dense.MlpBlock:
  lora_intermediate_conv = @mlp1/molora.MoLoRa()
  lora_output_conv = @mlp2/molora.MoLoRa()

mlp1/molora.MoLoRa:
  rank = 4
  num_experts = 10
  output_dim = 2816
  router = @mlp1/routing.Router()
  dtype = 'bfloat16'
  lora_axis_names_A = ('expert', 'embed', 'rank')
  lora_axis_names_B = ('expert', 'rank', 'mlp')

mlp2/molora.MoLoRa:
  rank = 4
  num_experts = 10
  output_dim = 1024
  router = @mlp2/routing.Router()
  dtype = 'bfloat16'
  lora_axis_names_A = ('expert', 'mlp', 'rank')
  lora_axis_names_B = ('expert', 'rank', 'embed')

attn/routing.Router:
  router_weights = @attn/routing.RouterWeights()
  input_axis_names = ('batch', 'length', 'embed')
  jitter_noise = 0.0
  dtype = 'float32'
  ignore_padding_tokens = False

attn/routing.RouterWeights:
  use_bias = False
  dtype = 'float32'
  kernel_axis_names = ('embed', 'expert')
  kernel_init = @router_init/linen.initializers.normal()
  bias_init = %BIAS_INIT
# We obtain slightly better results adopting typical normally-distributed
# scaling for the router, rather than the 0.1-scaled variance_scaling. May be
# worth revisiting if stability becomes an issue during training.
router_init/linen.initializers.normal:
  stddev = 2e-2

attn_out/routing.Router:
  router_weights = @attn_out/routing.RouterWeights()
  input_axis_names = ('batch', 'length', 'embed')
  jitter_noise = 0.0
  dtype = 'float32'
  ignore_padding_tokens = False

attn_out/routing.RouterWeights:
  use_bias = False
  dtype = 'float32'
  kernel_axis_names = ('embed', 'expert')
  kernel_init = @router_init/linen.initializers.normal()
  bias_init = %BIAS_INIT
# We obtain slightly better results adopting typical normally-distributed
# scaling for the router, rather than the 0.1-scaled variance_scaling. May be
# worth revisiting if stability becomes an issue during training.
router_init/linen.initializers.normal:
  stddev = 2e-2

mlp1/routing.Router:
  router_weights = @mlp1/routing.RouterWeights()
  input_axis_names = ('batch', 'length', 'embed')
  jitter_noise = 0.0
  dtype = 'float32'
  ignore_padding_tokens = False

mlp1/routing.RouterWeights:
  use_bias = False
  dtype = 'float32'
  kernel_axis_names = ('embed', 'expert')
  kernel_init = @router_init/linen.initializers.normal()
  bias_init = %BIAS_INIT
# We obtain slightly better results adopting typical normally-distributed
# scaling for the router, rather than the 0.1-scaled variance_scaling. May be
# worth revisiting if stability becomes an issue during training.
router_init/linen.initializers.normal:
  stddev = 2e-2

mlp2/routing.Router:
  router_weights = @mlp2/routing.RouterWeights()
  input_axis_names = ('batch', 'length', 'mlp')
  jitter_noise = 0.0
  dtype = 'float32'
  ignore_padding_tokens = False

mlp2/routing.RouterWeights:
  use_bias = False
  dtype = 'float32'
  kernel_axis_names = ('mlp', 'expert')
  kernel_init = @router_init/linen.initializers.normal()
  bias_init = %BIAS_INIT
# We obtain slightly better results adopting typical normally-distributed
# scaling for the router, rather than the 0.1-scaled variance_scaling. May be
# worth revisiting if stability becomes an issue during training.
router_init/linen.initializers.normal:
  stddev = 2e-2