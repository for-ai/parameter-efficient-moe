# ginlint: disable=bad-import-order
from __gin__ import dynamic_registration

import seqio
from t5x import models
from t5x import utils
from t5x import adafactor
from t5x import optimizers as optim
from flax import linen
from flax import traverse_util
from flaxformer.components import dense
from flaxformer.components.attention import dense_attention

from t5x import partitioning
from src import adafactor_custom as c_optim
from src import partitioning_custom as c_partitioning

from src import utils as peft_utils
from src import ia3

include 't5x/configs/runs/eval.gin'

# ========== Data Mixture ==========
# SeqIO tasks for p3 (from original repo)
import t0_data

# ========== These are IA3 HPs you might want to override ==========
# If you want to change the actual optimizer itself (to optim.Adam, etc), make
# sure to update the optimizer that is passed to the MultiOptimizer.
adafactor.Adafactor:
  decay_rate = 0.8
  step_offset = 0
  logical_factor_rules = @c_optim.standard_logical_factor_rules()

# ========== These are IA3 HPs you might want to override ==========
partitioning.PjitPartitioner:
  logical_axis_rules = @partitioning.standard_logical_axis_rules()

partitioning.standard_logical_axis_rules:
  additional_rules = @c_partitioning.standard_logical_axis_rules()

# ========== ARCHITECTURE ==========
# Add ia3 to all attention implementations
dense_attention.MultiHeadDotProductAttention:
  k_conv = @ia3.IA3Attention()
  v_conv = @ia3.IA3Attention()

ia3.IA3Attention:
  dtype = %ACTIVATION_DTYPE

dense.MlpBlock:
  intermediate_conv = @ia3.IA3()

ia3.IA3:
  axis_name = ('mlp',)
  dtype = %ACTIVATION_DTYPE
